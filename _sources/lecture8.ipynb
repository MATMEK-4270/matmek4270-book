{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8\n",
    "\n",
    "## Function approximation with global functions\n",
    "\n",
    "Consider a generic function in one dimension $u(x)$. $u(x)$ can be anything, a polynomial, harmonic, exponential, etc., and it can be defined on the entire real line $x \\in \\mathbb{R}$ or simply a part of it $x \\in [a, b]$ for some real numbers $a < b$. Sometimes you will see a notation like $u(x): [a, b] \\rightarrow \\mathbb{R}$. This means that the function $u(x)$ is defined on the domain $x\\in[a,b]$ and outputs a real number.   \n",
    "\n",
    "We now want to approximate this function $u(x)$ using other functions $\\psi_j(x)$, such that\n",
    "\n",
    "$$\n",
    "u(x) \\approx \\sum_{k=0}^N \\hat{u}_k \\psi_k(x).\n",
    "$$\n",
    "\n",
    "The other functions $\\psi_j(x)$ are called basis functions. The basis functions can also be anything, but they need to output the same kind of number as $u(x)$. That is, if $u(x)$ is a complex function, then $\\psi_j(x)$ better be  a complex function as well.\n",
    "\n",
    "A basis is a collection (or set) of basis functions\n",
    "\n",
    "$$\n",
    "\\{\\psi_j(x)\\}_{j=0}^N.\n",
    "$$\n",
    "\n",
    "A functionspace $V_N$ can be defined as the span of a this set of basis functions\n",
    "\n",
    "$$\n",
    "V_N = \\text{span}\\{\\psi_j\\}_{j=0}^N.\n",
    "$$\n",
    "\n",
    "For example, $\\text{span}\\{x^j\\}_{j=0}^{N}$ is the space of all polynomials of order less than or equal to $N$, which usually is denoted as $\\mathbb{P}_N$.\n",
    "\n",
    "If we say that the function $u_N \\in V_N$, this means that we can write\n",
    "\n",
    "$$\n",
    "u_N(x) = \\sum_{j=0}^N \\hat{u}_j \\psi_j(x).\n",
    "$$\n",
    "\n",
    "The set $\\boldsymbol{\\hat{u}} = \\{\\hat{u}_j\\}_{j=0}^N$ contains the expansion coefficients. These coefficients are the unknown when we are working with function approximation, or later, the finite element method.\n",
    "\n",
    "Consider a very simple example. Let $u(x) = 10(x-1)^2-1$ for $x\\in [1,2]$. We can create the function in Sympy and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "u = 10*(x-1)**2-1\n",
    "N = 100\n",
    "xj = np.linspace(1, 2, N)\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(xj, sp.lambdify(x, u)(xj));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a functionspace consisting of all straight lines\n",
    "\n",
    "$$\n",
    "V_N = \\text{span}\\{1, x\\},\n",
    "$$\n",
    "\n",
    "and try to find the approximation $u_N \\in V_N$ that is the best approximation of $u(x)$. How can we do this? All we know is that\n",
    "\n",
    "$$\n",
    "u_N(x) = \\hat{u}_0 + \\hat{u}_1 x.\n",
    "$$\n",
    "\n",
    "How do we find $\\{\\hat{u}_0, \\hat{u}_1\\}$ such that $u_N(x)$ is the best possible approximation to $u(x)$? \n",
    "\n",
    "There are several possible approaches. The three approaches considered in the [book](https://link-springer-com.ezproxy.uio.no/book/10.1007/978-3-030-23788-2) are \n",
    "\n",
    "* The least squares method (variational)\n",
    "* The Galerkin method (variational)\n",
    "* The collocation method (interpolation)\n",
    "\n",
    "where the first two are variational methods and the last is an interpolation method. We will briefly mention the least squares method and focus on the last two. All methods will allow us to find the best possible $u_N(x)$, but with slightly different approaches. All methods will provide us with $N+1$ equations that can be used to find the $N+1$ unknown $\\{\\hat{u}_j\\}_{j=0}^N$.\n",
    "\n",
    "For the first two methods we need to define the $L^2$ inner product\n",
    "\n",
    "$$\n",
    "\\left(f, g \\right) = \\int_{\\Omega} f(x)g(x) d\\Omega,\n",
    "$$(eq-L2-inner)\n",
    "\n",
    "for two real functions $f(x)$ and $g(x)$ (complex functions have a slightly different inner product). The symbol $\\Omega$ here represents the domain of interest. For our first example $\\Omega = [1, 2]$. \n",
    "\n",
    "```{note}\n",
    "Sometimes the inner product is written as $\\left(f, g \\right)_{L^2(\\Omega)}$, in order to clarify that it is the $L^2$ inner product on a certain domain. Sometimes the inner product is also called the scalar product.\n",
    "```\n",
    "\n",
    "The inner product is used in defining the $L^2$ error norm. If we define the error in the approximation as\n",
    "\n",
    "$$\n",
    "e(x) = u(x)-u_N(x),\n",
    "$$(eq-error-u)\n",
    "\n",
    "then the $L^2$ error norm is $\\sqrt{(e, e)} = |e|$, or sometimes $|e|_{L^2}$. We will also use\n",
    "\n",
    "$$\n",
    "E = (e, e).\n",
    "$$\n",
    "\n",
    "For convenience we define a Python function for the inner product of Sympy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(u, v, domain=(-1, 1), x=x):\n",
    "    return sp.integrate(u*v, (x, domain[0], domain[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The least squares method \n",
    "\n",
    "The least squares method requires that we solve\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\hat{u}_j} = 0, \\quad j \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "This gives us $N+1$ equations for the $N+1$ unknown $(\\hat{u}_j)_{j=0}^{N}$. For our example, this is a lot of work by hand, but easily achieved using Sympy.\n",
    "\n",
    "Find and display the two equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "u0, u1 = sp.symbols('u0,u1')\n",
    "err = u-(u0+u1*x)\n",
    "E = inner(err, err, (1, 2))\n",
    "eq1 = sp.Eq(sp.diff(E, u0, 1), 0)\n",
    "eq2 = sp.Eq(sp.diff(E, u1, 1), 0)\n",
    "display(eq1)\n",
    "display(eq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the two equations for the two unknown `u0` and `u1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uhat = sp.solve((eq1, eq2), (u0, u1))\n",
    "uhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot the function $u(x)=10(x-1)^2-1$ and the best approximation $u_N(x) \\in \\text{span}\\{1, x\\}$ according to the least squares method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(xj, sp.lambdify(x, u)(xj), 'b', xj, uhat[u0]+uhat[u1]*xj, 'r:')\n",
    "plt.legend(['$u(x)$', '$u_{N}(x)$']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Galerkin method\n",
    "\n",
    "The Galerkin method requires that the error is orthogonal to the basis functions (for the $L^2$ inner product). That is\n",
    "\n",
    "$$\n",
    "(e, \\psi_j) = 0, \\quad \\forall \\, j \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "This is not as much work as the least squares method, and even easier to implement in Sympy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq1 = sp.Eq(inner(err, 1, (1, 2)), 0)\n",
    "eq2 = sp.Eq(inner(err, x, (1, 2)), 0)\n",
    "display(eq1)\n",
    "display(eq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.solve((eq1, eq2), (u0, u1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "For function approximations the Galerkin method always gives the same result as the least squares method. However, the same does not apply when solving PDEs.\n",
    "```\n",
    "\n",
    "The Galerkin method can be formulated as:\n",
    "\n",
    "Find $u_N \\in V_N$ such that\n",
    "\n",
    "$$\n",
    "(e, v) = 0, \\quad \\forall \\, v \\in V_N,\n",
    "$$(eq:Galerkin_a)\n",
    "\n",
    "which means exactly the same as:\n",
    "\n",
    "Find $u_N \\in V_N$ such that\n",
    "\n",
    "$$\n",
    "(e, \\psi_i) = 0, \\quad \\forall \\, i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$(eq:Galerkin_b)\n",
    "\n",
    "```{note}\n",
    "In order to satisfy $(e, v)=0$ for all $v \\in V_N$, we can insert for $v=\\sum_{j=0}^N\\hat{v}_j \\psi_j$ such that \n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^N (e, \\psi_j) \\hat{v}_j =0.\n",
    "$$\n",
    "In order for this to always be satisfied, we require that $(e, \\psi_j)=0$ for all $j=0,1,\\ldots, N$. Hence the equality between the two above formulations {eq}`eq:Galerkin_a` and {eq}`eq:Galerkin_b`.\n",
    "```\n",
    "```{note}\n",
    "The Galerkin method described above is sometimes referred to as a projection method and may be formulated as: find $u_N$(x) as the projection of $u(x)$ into $V_N$. \n",
    "```\n",
    "\n",
    "Inserting for $e=u-u_N$ and $u_N=\\sum_{j=0}^N \\hat{u}_j \\psi_j$ we get\n",
    "\n",
    "$$\n",
    "(u - \\sum_{j=0}^N \\hat{u}_j \\psi_j, \\psi_i) = 0, \\quad \\forall \\, i \\in \\{0, 1, \\ldots, N\\},\n",
    "$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^N (\\psi_j, \\psi_i) \\, \\hat{u}_j = (u, \\psi_i), \\quad \\forall \\, i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "This is a linear algebra system, \n",
    "\n",
    "$$\n",
    "A \\boldsymbol{x} = \\boldsymbol{b},\n",
    "$$\n",
    "\n",
    "where the matrix $A = (a_{ij})_{i,j=0}^N, a_{ij} = (\\psi_j, \\psi_i)$ and the vectors $\\boldsymbol{x} = (\\hat{u}_i)_{i=0}^N$ and $\\boldsymbol{b}  = (b_i)_{i=0}^N, b_i = (u, \\psi_i)$.\n",
    "\n",
    "```{note}\n",
    "The matrix $A$, including only the basis functions and no derivatives, is often called the *mass matrix*.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "In the Galerkin literature, the basis function containing the row index (i.e., $\\psi_i$) is usually referred to as the *test* function, whereas the basis function containing the column index (i.e., $\\psi_j$) is referred to as the *trial* function. Sometimes the entire $u_N(x)$ is called the trial function. The function $v$ above is a test function.\n",
    "```\n",
    "\n",
    "If the basis functions are chosen as orthonormal to each other, then\n",
    "\n",
    "$$\n",
    "(\\psi_j, \\psi_i) = \\delta_{ij} = \\begin{cases}\n",
    "1 \\text{ if } i = j, \\\\\n",
    "0 \\text{ if } i \\ne j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In that case we simply get\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x} = \\boldsymbol{b} \\quad \\text{or} \\quad \\hat{u}_i = (u, \\psi_i) \\quad \\forall \\, i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is more common that the basis functions are *orthogonal*. In this case the mass matrix becomes\n",
    "\n",
    "$$\n",
    "(\\psi_j, \\psi_i) = |\\psi_i|^2 \\delta_{ij},\n",
    "$$\n",
    "\n",
    "where $|\\psi_i|^2$ is the squared $L^2$ norm of $\\psi_i$. We can still easily solve the linear algebra system (because $A$ is a diagonal matrix)\n",
    "\n",
    "$$\n",
    "\\hat{u}_i = \\frac{(u, \\psi_i)}{|\\psi_i|^2} \\quad \\forall \\, i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "The monomial basis\n",
    "\n",
    "$$\n",
    "\\{x^n | n \\in \\mathbb{N} \\},\n",
    "$$\n",
    "\n",
    "is a basis for the space of all polynomials. However, it is not a good basis. This is because the basis functions are not orthogonal and the mass matrix $A$ is ill conditioned.\n",
    "\n",
    "A much better polynomial basis are the Legendre polynomials that are defined on the domain $\\Omega = [-1, 1]$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P_0(x) &= 1, \\\\\n",
    "P_1(x) &= x, \\\\\n",
    "P_2(x) &= \\frac{1}{2}(3x^2-1), \\\\\n",
    "&\\vdots \\\\\n",
    "(j+1)P_{j+1}(x) &= (2j+1)xP_{j}(x) - j P_{j-1}(x).\n",
    "\\end{align*}\n",
    "$$(eq-Legrecursive)\n",
    "\n",
    "The first 5 Legendre polynomials are plotted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "xj = np.linspace(-1, 1, 100)\n",
    "legend = []\n",
    "p = np.zeros(100)\n",
    "for n in range(5):\n",
    "    l = sp.legendre(n, x)\n",
    "    p[:] = sp.lambdify(x, l)(xj)\n",
    "    plt.plot(xj, p)\n",
    "    legend.append(f'$P_{n}(x)$')\n",
    "plt.legend(legend);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Legendre polynomials are $L^2$ orthogonal on $\\Omega=[-1, 1]$\n",
    "\n",
    "$$\n",
    "(P_j, P_i) =  \\frac{2}{2j+1}\\delta_{ij}.\n",
    "$$ (eq-legortho)\n",
    "\n",
    "A slightly complicating factor, though, is that the computational domain needs to be $[-1, 1]$. So if the actual domain of $u(x)$ is $[a, b]$, then this needs to be mapped to the computational domain $[-1, 1]$. \n",
    "\n",
    "```{note}\n",
    "Mapping from a physical to a computational (reference) domain is very common and it is best to get used to it sooner rather than later. With the finite element method this mapping is used all the time and it is described as one of the first topics for the method in Sec 3.1.8 of [Introduction to Numerical Methods for Variational Problems](https://link-springer-com.ezproxy.uio.no/book/10.1007/978-3-030-23788-2).\n",
    "```\n",
    "\n",
    "```{note}\n",
    "The Legendre polynomials are not the only basis functions that require a mapping. For example, the Bernstein polynomials (see, Sec. 2.4.3 in the [book](https://link-springer-com.ezproxy.uio.no/book/10.1007/978-3-030-23788-2)) use a computational domain $[0, 1]$. The computational domain depends on the chosen basis function.\n",
    "```\n",
    "\n",
    "Let $X$ be the coordinate in the computational (reference) domain $[A, B]$, where $A$ and $B$ are real numbers. A linear (affine) mapping, from the computational coordinate $X$ to the physical coordinate $x$ is then\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x = a + \\frac{b-a}{B-A}(X-A).\n",
    "\\end{equation}\n",
    "$$(eq-x)\n",
    "\n",
    "The reverse mapping is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X = A + \\frac{B-A}{b-a}(x-a).\n",
    "\\end{equation}\n",
    "$$(eq-xmap)\n",
    "\n",
    "These mappings are valid for any basis function. \n",
    "\n",
    "```{note}\n",
    "When using a basis function like $\\psi_k(x) = \\sin((k+1) \\pi x), x \\in [0, 1]$, you are implicitly mapping the physical domain $[0, 1]$ to an integer $k+1$ times a computational domain $[0, \\pi]$. \n",
    "```\n",
    "\n",
    "For Legendre basis functions we use the computational domain $[-1, 1]$, so $A=-1$ and $B=1$. With this mapping we then simply use the basis functions\n",
    "\n",
    "$$\n",
    "\\psi_j(x) = P_{j}(X(x)).\n",
    "$$\n",
    "\n",
    "We need to be careful with defining the $L^2$ inner product though, since this requires integration with a change of variables. Consider the Galerkin method on the domain $[a, b]$\n",
    "\n",
    "$$\n",
    "(u(x)-u_N(x), \\psi_i(x)) = \\int_{a}^b (u(x)-u_N(x)) \\psi_i(x) dx = 0, \\quad \\forall i \\in \\{0, 1, \\ldots, N\\}. \n",
    "$$\n",
    "\n",
    "Insert for $u_N(x)=\\sum_{j=0}^N \\hat{u}_j \\psi_j(x)$  and rearrange into\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^N \\int_{a}^b \\psi_j(x)  \\psi_i(x) \\, dx \\, \\hat{u}_j =  \\int_{a}^b u(x) \\psi_i(x) dx, \\quad \\forall i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "This is just the regular Galerkin method. But now we introduce $\\psi_j(x) = P_j(X)$ and a change of variables for the integration. This leads to\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^N \\int_{-1}^1 P_j(X)  P_i(X)\\, \\frac{dx}{dX} \\, dX \\, \\hat{u}_j =  \\int_{-1}^1 u(x(X)) P_i(X) \\, \\frac{dx}{dX} \\, dX, \\quad \\forall i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "Note that $\\frac{dx}{dX}$ is a constant and since it appears on both sides it cancels out. We can thus write this as\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^N  (P_j(X), P_i(X))_{L^2([-1,1])} \\, \\hat{u}_j = (u(x(X)), P_i(X))_{L^2([-1,1])}\n",
    "$$\n",
    "\n",
    "where $L^2([-1, 1])$ is written out to highlight that this inner product is over the domain $[-1, 1]$. Since $(P_j, P_i)_{L^2([-1,1])} = \\frac{2}{2j+1}\\delta_{ij}$ we get\n",
    "\n",
    "$$\n",
    "\\hat{u}_i = \\frac{2i+1}{2} \\left( u(x(X)), P_i(X) \\right)_{L^2([-1,1])} \\quad \\forall \\, i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "We can rewrite the `inner` function to work for a generic interval $[a, b]$. Note that it is only the function $u(x)$ that requires mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(u, v, domain, ref_domain=(-1, 1)):\n",
    "    A, B = ref_domain\n",
    "    a, b = domain\n",
    "    us = u.subs(x, a + (b-a)*(x-A)/(B-A))\n",
    "    return sp.integrate(us*v, (x, A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for our $u(x)=10(x-1)^2-1$ in the domain $\\Omega = [a, b] = [1, 2]$ we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 1, 2\n",
    "uhat = lambda u, j: (2*j+1) * inner(u, sp.legendre(j, x), (a, b))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute any Legendre coefficient from the above lambda function. For $u(x)=10(x-1)^2-1$ and $j=0$ and $j=1$ we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uhat(10*(x-1)**2-1, 0), uhat(10*(x-1)**2-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus plot the function \n",
    "\n",
    "$$\n",
    "u_N(x) = \\frac{7}{3}P_0(X) + 5 P_1(X)\n",
    "$$\n",
    "\n",
    "and compare with the exact $u(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import Legendre\n",
    "plt.figure(figsize=(4, 3))\n",
    "xj = np.linspace(1, 2, 100)\n",
    "Xj = -1 + 2/(b-a)*(xj-a)\n",
    "plt.plot(xj, sp.lambdify(x, u)(xj))\n",
    "plt.plot(xj, uhat(u, 0) + uhat(u, 1)*Xj, 'r:')\n",
    "plt.legend(['$10(x-1)^2-1$', f'{Legendre((uhat(u, 0), uhat(u, 1)))}']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the result is exactly as before, which is not so strange since the span of this Legendre basis is still the space of all linear functions. But add one more Legendre coefficient and the approximation becomes exact because $u(x)$ is a second order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = sp.lambdify(x, sp.legendre(2, x))\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(xj, sp.lambdify(x, u)(xj))\n",
    "plt.plot(xj, uhat(u, 0) + uhat(u, 1)*Xj + uhat(u, 2)*l2(Xj), 'r:')\n",
    "plt.legend(['$10(x-1)^2-1$', f'{Legendre((uhat(u, 0), uhat(u, 1), uhat(u, 2)))}']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Adding one more Legendre coefficient does not change the first two.\n",
    "```\n",
    "```{note}\n",
    "Note that we have not used any mesh points when computing the Legendre coefficients $(\\hat{u}_j)_{j=0}^N$, only exact integrals.\n",
    "```\n",
    "\n",
    "In the computations above we have used Sympy and computed the inner product integral exactly. For some functions this may be impossible, or take a very long time. Hence, the inner products are often computed with numerical integration. We can rewrite the inner product above to use numerical integration (from scipy). The results are now almost identical, but no longer exact. See the difference below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ninner(u, v, domain, ref_domain=(-1, 1)):\n",
    "    from scipy.integrate import quad\n",
    "    A, B = ref_domain\n",
    "    a, b = domain\n",
    "    us = u.subs(x, a + (b-a)*(x-A)/(B-A))\n",
    "    uv = sp.lambdify(x, us*v)\n",
    "    return quad(uv, A, B)[0]\n",
    "\n",
    "uhatn = lambda u, j: (2*j+1) * ninner(u, sp.legendre(j, x), (1, 2))/2\n",
    "display((uhat(10*(x-1)**2-1, 0), uhatn(10*(x-1)**2-1, 0)))\n",
    "display((uhat(10*(x-1)**2-1, 1), uhatn(10*(x-1)**2-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical integration routine [quad](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html) is using adaptive quadrature in order to solve the integral. This adaptive integration routine is using a gradually finer mesh until the results of the integration are no longer improving more than a certain tolerance. The default tolerances (both relative and absolute) are $1.49 \\cdot 10^{-8}$, but may also be set by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The collocation method \n",
    "\n",
    "The collocation method takes a very different approach, but the objective is still to find $u_N \\in V_N$ such that\n",
    "\n",
    "$$\n",
    "u_N(x) = \\sum_{j=0}^N \\hat{u}_j \\psi_j(x).\n",
    "$$\n",
    "\n",
    "The collocation method requires that for some $N+1$ chosen mesh points $x_j$ the following $N+1$ equations are satisfied\n",
    "\n",
    "$$\n",
    "u(x_j) - u_N(x_j) = 0, \\quad j \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "Inserting for $u_N(x)$ we get the $N+1$ equations\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^N \\hat{u}_j \\psi_{j}(x_i) = u(x_i), \\quad i \\in \\{0, 1, \\ldots, N\\},\n",
    "$$\n",
    "\n",
    "which can be written as the linear algebra system\n",
    "\n",
    "$$\n",
    " \\sum_{j} a_{ij} \\hat{u}_j = u_i,\n",
    "$$\n",
    "\n",
    "where the matrix components $a_{ij} = \\psi_j(x_i)$ and $u_i = u(x_i)$.\n",
    "\n",
    "The Lagrange interpolation method described in [lecture 7](https://matmek-4270.github.io/matmek4270-book/lecture7.html#lagrange-interpolation-polynomials) is a collocation method. The Lagrange basis functions\n",
    "\n",
    "$$\n",
    "\\ell_j(x) = \\prod_{\\substack{0 \\le m \\le N \\\\ m \\ne j}} \\frac{x-x_m}{x_j-x_m},\n",
    "$$\n",
    "\n",
    "are defined such that\n",
    "\n",
    "$$\n",
    "\\ell_j(x_i) = \\delta_{ij} = \\begin{cases} 1 \\quad &\\text{for } i=j \\\\\n",
    "0 \\quad &\\text{for }  i\\ne j.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hence for the Lagrange interpolation method the matrix $A$ is the identity matrix and we can simply use the coefficients\n",
    "\n",
    "$$\n",
    "\\hat{u}_j = u(x_j).\n",
    "$$\n",
    "\n",
    "There is no integration and the method is often favoured for its simplicity. There is a problem though. How do you choose the collocation points?!\n",
    "\n",
    "Lets return to the example with $u(x)=10(x-1)^2-1$ and $x \\in [1, 2]$. The approximation using two collocation points (linear function) is now\n",
    "\n",
    "$$\n",
    "u_N(x) = \\hat{u}_0 \\ell_0(x) + \\hat{u}_1 \\ell_1(x),\n",
    "$$\n",
    "\n",
    "or more simply\n",
    "\n",
    "$$\n",
    "u_N(x) = u(x_0) \\ell_0(x) + u(x_1) \\ell_1(x).\n",
    "$$\n",
    "\n",
    "We can choose the end points $[x_0, x_1] = [1, 2]$ and reuse the two functions `Lagrangebasis` and `Lagrangefunction` from [lecture 7](https://matmek-4270.github.io/matmek4270-book/lecture7.html#lagrange-interpolation-polynomials). The result is then as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lagrange import Lagrangebasis, Lagrangefunction\n",
    "xj = np.linspace(1, 2, 100)\n",
    "u = 10*(x-1)**2-1\n",
    "ell = Lagrangebasis([1, 2])\n",
    "L = Lagrangefunction([u.subs(x, 1), u.subs(x, 2)], ell)\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(xj, sp.lambdify(x, u)(xj), 'b')\n",
    "plt.plot(xj, sp.lambdify(x, L)(xj), 'r:')\n",
    "plt.plot([1, 2], [u.subs(x, 1), u.subs(x, 2)], 'bo')\n",
    "plt.legend(['$u(x)$', '$(x_0, x_1) = (1, 2)$']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this seems to be less accurate than the Galerkin method or the least squares method. However, we have not done any integration, and the lower accuracy has come very easily. We can also very easily use one more point and end up with a perfect approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = Lagrangebasis([1, 1.5, 2])\n",
    "L = Lagrangefunction([u.subs(x, 1), u.subs(x, 1.5), u.subs(x, 2)], ell)\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(xj, sp.lambdify(x, u)(xj), 'b')\n",
    "plt.plot(xj, sp.lambdify(x, L)(xj), 'r:')\n",
    "plt.plot([1, 1.5, 2], [u.subs(x, 1), u.subs(x, 1.5), u.subs(x, 2)], 'bo')\n",
    "plt.legend(['$u(x)$', '$(x_0, x_1, x_2) = (1, 1.5, 2)$']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider a more difficult function\n",
    "\n",
    "$$\n",
    "u(x) = \\frac{1}{1+16x^2}, \\quad x \\in [-1, 1],\n",
    "$$\n",
    "\n",
    "and attempt to approximate it with Lagrange polynomials on a uniform grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 1/(1+16*x**2)\n",
    "N = 16\n",
    "xj = np.linspace(-1, 1, N+1)\n",
    "uj = sp.lambdify(x, u)(xj)\n",
    "ell = Lagrangebasis(xj)\n",
    "L = Lagrangefunction(uj, ell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot on a fine grid to plot in between mesh points. We know the Lagrange polynomial is exact on the mesh points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yj = np.linspace(-1, 1, 1000)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(xj, uj, 'bo')\n",
    "plt.plot(yj, sp.lambdify(x, u)(yj))\n",
    "plt.plot(yj, sp.lambdify(x, L)(yj), 'r:')\n",
    "ax = plt.gca()\n",
    "ax.set_ylim(-2, 1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that did not work out well! Large over and under-shoots in between the mesh points. So Lagrange polynomials are bad, right?\n",
    "\n",
    "It turns out that due to something called [Runge's phenomenon](https://en.wikipedia.org/wiki/Runge%27s_phenomenon) it is a very bad idea to interpolate on a uniform mesh.\n",
    "\n",
    "Lets try something else and use a clustering of points near the two edges. Chebyshev points are defined as\n",
    "\n",
    "$$\n",
    "x_j = \\cos(j \\pi / N), \\quad j \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "Use these mesh points for the Lagrange polynomials and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xj = np.cos(np.arange(N+1)*np.pi/N)\n",
    "uj = sp.lambdify(x, u)(xj)\n",
    "ell = Lagrangebasis(xj)\n",
    "L = Lagrangefunction(uj, ell)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(xj, uj, 'bo')\n",
    "plt.plot(yj, sp.lambdify(x, u)(yj))\n",
    "plt.plot(yj, sp.lambdify(x, L)(yj), 'r:');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agreement is now much better. And the agreement will become better and better the more points you use. Whereas for a uniform grid the agreement will become worse and worse the more points you use.\n",
    "\n",
    "How about Legendre polynomials? The domain is $[-1, 1]$ so we can get the Legendre coefficients without any mapping simply as\n",
    "\n",
    "$$\n",
    "\\hat{u}_i = \\frac{2i+1}{2}(u, P_i).\n",
    "$$(eq-legcoeff)\n",
    "\n",
    "Compute the first 40 Legendre coefficients and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uj = lambda u, j: (2*j+1) * inner(sp.legendre(j, x), u, (-1, 1))/2\n",
    "ul = []\n",
    "for n in range(40):\n",
    "    ul.append(uj(u, n).n())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 2))\n",
    "plt.semilogy(ul, '+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Legendre coefficients are decreasing. This is because the series is converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-boundary-issues)=\n",
    "### Boundary issues \n",
    "\n",
    "Lets consider a slightly different problem\n",
    "\n",
    "$$\n",
    "u(x) = 10(x-1)^2 -1, \\quad x \\in [0, 1]\n",
    "$$\n",
    "\n",
    "and attempt to find $u_N \\in V_N = \\text{span}\\{\\sin((j+1)\\pi x)\\}_{j=0}^N$ with the Galerkin method.\n",
    "\n",
    "The sines are orthogonal such that the mass matrix becomes diagonal\n",
    "\n",
    "$$\n",
    "\\int_0^1 \\sin((j+1) \\pi x) \\sin((i + 1) \\pi x) dx = \\frac{1}{2} \\delta_{ij}.\n",
    "$$\n",
    "\n",
    "Hence we can easily get the coefficients with the Galerkin method\n",
    "\n",
    "$$\n",
    "\\hat{u}_i = 2 \\left( u(x),  \\sin((i+1) \\pi x) \\right)_{L^2([0, 1])}.\n",
    "$$(eq-hatusin)\n",
    "\n",
    "Lets implement and check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 10*(x-1)**2-1\n",
    "uhat = lambda u, i: 2*inner(u, sp.sin((i+1)*sp.pi*x), (0, 1), (0, 1))\n",
    "ul = []\n",
    "for i in range(15):\n",
    "    ul.append(uhat(u, i).n())\n",
    "\n",
    "ul = np.array(ul, dtype=float)\n",
    "def uN(uh, xj):\n",
    "    N = len(xj)\n",
    "    uj = np.zeros(N)\n",
    "    for i, ui in enumerate(uh):\n",
    "        uj[:] += ui*np.sin((i+1)*np.pi*xj)\n",
    "    return uj\n",
    "\n",
    "xj = np.linspace(0, 1, 100)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(xj, 10*(xj-1)**2-1, 'b')\n",
    "plt.plot(xj, uN(ul[:(3+1)], xj), 'g:')\n",
    "plt.plot(xj, uN(ul[:(12+1)], xj), 'r-')\n",
    "plt.legend(['$u(x)$', 'N=3', 'N=12']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, we get large oscillations and the solution does not seem to converge. The problem is that the basis functions are all such that\n",
    "\n",
    "$$\n",
    "\\psi_j(0) = \\psi_j(1) = 0,\n",
    "$$\n",
    "\n",
    "since $\\sin((j+1) \\pi x) = 0$ for all $j\\in \\mathbb{N}$ and $x =0$ or $x=1$.  So no matter how many basis functions and unknown we're adding, the sum\n",
    "\n",
    "$$\n",
    "u_N(x) = \\sum_{j=0}^N \\hat{u}_j \\sin((j+1) \\pi x),\n",
    "$$\n",
    "\n",
    "will always lead to $u_N(0) = 0$ and $u_N(1) = 0$. We have a problem with the basis functions having the wrong values at the boundaries.\n",
    "\n",
    "A possible solution to this problem is to add a function to the right hand side above that satisfies the boundary conditions:\n",
    "\n",
    "$$\n",
    "u_N(x) = B(x) + \\sum_{j=0}^N \\hat{u}_j \\sin((j+1) \\pi x),\n",
    "$$(eq-uN0)\n",
    "\n",
    "where $B(0) = u(0)$ and $B(1) = u(1)$. This automatically leads to the correct solution $u_N(0) = B(0) = u(0)$ and $u_N(1)=B(1)=u(1)$. For our example we can simply use use $B(x) = u(0)(1-x) + u(1)x$. Hence, there are no new unknowns.\n",
    "\n",
    "```{note}\n",
    "The function $B(x)$ can always be fixed using two straight lines. However, the exact form will depend on the domain. \n",
    "```\n",
    "\n",
    "We get the slightly untraditional variational problem: find $u_N \\in V_N$ such that (note that $u_N$ below is only the sum and thus not equal to $u_N$ in {eq}`eq-uN0`)\n",
    "\n",
    "$$\n",
    "(u_N+B-u, v) = 0, \\quad \\forall \\, v \\in V_N,\n",
    "$$\n",
    "\n",
    "where $V_N = \\text{span}\\{\\sin((j+1)\\pi x)\\}_{j=0}^N$. We get\n",
    "\n",
    "$$\n",
    "\\left(\\sum_{j=0}^N \\hat{u}_j \\sin((j+1)\\pi x)+B-u, \\sin((i+1)\\pi x) \\right) = 0, \\quad i \\in \\{0, 1, \\ldots, N\\},\n",
    "$$\n",
    "\n",
    "and using the orthogonality of the sines we get\n",
    "\n",
    "$$\n",
    "\\hat{u}_i = 2 \\left(u-B, \\sin((i+1)\\pi x) \\right) \\quad i \\in \\{0, 1, \\ldots, N\\}.\n",
    "$$\n",
    "\n",
    "Compared with Eq. {eq}`eq-hatusin` this is only a minor modification, but it makes a world of difference. Lets implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc = []\n",
    "b = u.subs(x, 0)*(1-x) + u.subs(x, 1)*x\n",
    "for i in range(15):\n",
    "    uc.append(uhat(u-b, i).n())\n",
    "\n",
    "uc = np.array(uc, dtype=float)\n",
    "def uNc(uh, xj):\n",
    "    N = len(xj)\n",
    "    uj = u.subs(x, 0)*(1-xj) + u.subs(x, 1)*xj\n",
    "    for i, ui in enumerate(uh):\n",
    "        uj[:] += ui*np.sin((i+1)*np.pi*xj)\n",
    "    return uj\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(xj, 10*(xj-1)**2-1, 'b')\n",
    "plt.plot(xj, uNc(uc[:(3+1)], xj), 'g:')\n",
    "plt.plot(xj, uNc(uc[:(12+1)], xj), 'r-')\n",
    "plt.legend(['$u(x)$', 'N=3', 'N=12']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the very good agreement. The expansion coefficients are now converging, which we can see by printing out the 15 computed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These should be compared with the expansion coefficients without the boundary adjustment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sine basis functions need modification because they are are all zero on the boundary. \n",
    "Legendre polynomials, on the other hand, can be used without any boundary adjustments since\n",
    "\n",
    "$$\n",
    "P_j(-1) = (-1)^j \\quad \\text{and} \\quad P_j(1) = 1.\n",
    "$$\n",
    "\n",
    "We get a minor modification to the Legendre coefficients because of the new physical domain, but otherwise there is no issue and again we get a perfect approximation using three Legendre polynomials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0, 1\n",
    "uhat = lambda u, j: (2*j+1) * inner(u, sp.legendre(j, x), (0, 1))/2\n",
    "uL = []\n",
    "for i in range(6):\n",
    "    uL.append(uhat(u, i))\n",
    "print(uL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weekly assignments\n",
    "\n",
    "Experiment with the Galerkin and collocation methods and approximate the global functions\n",
    "\n",
    "1. $u(x) = |x|, \\quad x \\in [-1, 1]$\n",
    "2. $u(x) = \\exp(\\sin(x)), \\quad x \\in [0, 2]$\n",
    "3. $u(x) = x^{10}, \\quad x \\in [0, 1]$\n",
    "4. $u(x) = \\exp(-(x-0.5)^2) - \\exp(-0.25) \\quad x \\in [0, 1]$\n",
    "5. $u(x) = J_0(x), \\quad x \\in [0, 100]$\n",
    "\n",
    "where $J_0(x)$ is the [Bessel function]() of the first kind. The Bessel function is available both in [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.jv.html#scipy.special.jv) and [Sympy](https://docs.sympy.org/latest/modules/functions/special.html#sympy.functions.special.bessel.besselj).\n",
    "\n",
    "Experiment using either Legendre polynomials or sines as basis functions for the Galerkin method and Lagrange polynomials for the collocation method. Use both exact and numerical integration for the Galerkin method. Measure the error as a function of $N$ by computing an $L^2$ error norm.\n",
    "\n",
    "```{hint}\n",
    "Number 5 is difficult because of several factors. i) The Bessel function is expensive to integrate exactly and you need to use numerical integration. ii) It requires approximately 80 Legendre coefficients to completely converge. The Legendre polynomials returned by Sympy are such that when simply lambdified and evaluated, they will lead to severe roundoff errors. As such, it is better to use [Numpy's Legendre class](https://numpy.org/doc/stable/reference/routines.polynomials.legendre.html) for evaluation in the numerical integral.\n",
    "```\n",
    "\n",
    "The approximation of the Bessel function with Legendre polynomials is shown below for $N=(20, 40, 60)$. For $N=60$ there is no visible difference from the exact solution.\n",
    "\n",
    "![Approximation of the Bessel function](Bessel.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
